base_config: &BASE_CONFIG

    # metadata file for the dataset
    metadata_json_path: ""

    # data 
    train_data_path: ""
    valid_data_path: ""
    exp_dir: ""
    n_years: 1
    img_shape_y: 1440
    img_shape_x: 721

    # files used for normalization of the data
    min_path: ""
    max_path: ""
    time_means_path:   ""
    global_means_path: ""
    global_stds_path:  ""
    time_diff_means_path: ""
    time_diff_stds_path: ""

    # architecture related 
    nettype: "SFNO"
    model_grid_type: "equiangular"
    sht_grid_type: "legendre-gauss"
    filter_type: "linear"
    scale_factor: 3
    embed_dim: 384
    num_layers: 8
    complex_activation: "real"
    normalization_layer: "instance_norm"
    hard_thresholding_fraction: 1.0
    use_mlp: !!bool True
    mlp_mode: "serial"
    mlp_ratio: 2
    separable: !!bool False
    operator_type: "dhconv"
    activation_function: "gelu"
    pos_embed: "none" # "none", "direct" or "frequency". For resoltuion invariance, use add_grid instead or use "frequency"

    # training parameters
    loss: "absolute squared geometric l2"
    channel_weights: "auto"
    lr: 1E-3
    n_eval_samples: 8760
    max_epochs: 500
    batch_size: 1 # must be the N * num(gpus) originally 64 lkkbox250429
    weight_decay: 0.0

    # scheduler parameters
    scheduler: "StepLR" # "ReduceLROnPlateau" or "CosineAnnealingLR"
    scheduler_T_max: 70
    scheduler_factor: 0.1
    scheduler_patience: 10
    scheduler_step_size: 100
    scheduler_gamma: 0.5
    lr_warmup_steps: 0

    # general
    verbose: !!bool False

    # wireup stuff
    wireup_info: "mpi"
    wireup_store: "tcp"

    num_data_workers: 1
    num_visualization_workers: 1
    dt: 1 # how many timesteps ahead the model will predict
    n_history: 0 # how many previous timesteps to consider
    prediction_type: "iterative"
    prediction_length: 35 # applicable only if prediction_type == "iterative"
    n_initial_conditions: 5 # applicable only if prediction_type == "iterative"
    valid_autoreg_steps: 19 # number of autoregressive steps for validation, 20 steps in total

    # we make the "epochs" shorter so we have more reporting
    n_train_samples_per_epoch: 54000

    ics_type: "specify_number"
    save_raw_forecasts: !!bool True
    save_channel: !!bool False
    masked_acc: !!bool False
    maskpath: None
    perturb: !!bool False
    add_noise: !!bool False
    noise_std: 0.

    target: "default" # options default, residual
    normalize_residual: false

    # define channels to be read from data
    # channel_names: ["u10m", "v10m", "u100m", "v100m", "t2m", "sp", "msl", "tcwv", "u50", "u100", "u150", "u200", "u250", "u300", "u400", "u500", "u600", "u700", "u850", "u925", "u1000", "v50", "v100", "v150", "v200", "v250", "v300", "v400", "v500", "v600", "v700", "v850", "v925", "v1000", "z50", "z100", "z150", "z200", "z250", "z300", "z400", "z500", "z600", "z700", "z850", "z925", "z1000", "t50", "t100", "t150", "t200", "t250", "t300", "t400", "t500", "t600", "t700", "t850", "t925", "t1000", "q50", "q100", "q150", "q200", "q250", "q300", "q400", "q500", "q600", "q700", "q850", "q925", "q1000"]
    # channel_names: ["u10m", "v10m", "t2m", "sp", "msl", "tcwv", "u1000", "v1000", "z1000", "t850", "u850", "v850", "z850", "r850", "t500", "u500", "v500", "z500", "r500", "z500", "z50", "tcwv"]
    normalization: "zscore" # options zscore or minmax or none

    # extra channels
    add_grid: !!bool False
    gridtype: "sinusoidal"
    grid_num_frequencies: 16
    roll: !!bool False
    add_zenith: !!bool True
    # invariants
    add_orography: !!bool True
    orography_path: "/work/lkkbox945/models/makani/v250508/datasets/source/invariant/orography.nc"
    add_landmask: !!bool True
    #landmask_path: "/work/lkkbox945/models/makani/v250508/datasets/source/invariant/land_sea_mask.nc"
    landmask_path: "/home/choutilin1/makani/datasets/source/invariant/land_sea_mask.nc"

    finetune: !!bool False

    # logging options
    log_to_screen: !!bool True
    log_to_wandb: !!bool False 
    log_video: 0 # if > 0 will log every i-th epoch
    save_checkpoint: "legacy"

    optimizer_type: "AdamW"
    optimizer_beta1: 0.9
    optimizer_beta2: 0.95
    optimizer_max_grad_norm: 32
    crop_size_x: None
    crop_size_y: None

    # required for validation and scoring
    inf_data_path: ""

    # Weights and biases configuration
    wandb_name: None # If None, wandb will assign a random name, recommended
    wandb_group: "sfnonet development" # If None, will be "era5_wind" + config, but you can override it here
    wandb_project: "sfno architecture validation"
    wandb_entity: "sfno-large-model-training"

predict_base: &PREDICT_BASE
    predict_with_best_ckpt: !!bool True # otherwise use the checkpoint of the latest epoch
    predict_output_skipExists: !!bool True
    predict_output_overwrite: !!bool False
    predict_output_dir: "./"     # [str]
    predict_ic_path: './'           # [str]
    predict_ic_mode: 'incontinuous' # ["continuous"], ["incontinuous"]
    predict_ic_start: null          # [null], [int]
    predict_ic_stop: null          # [null], [int]
    predict_ic_step: null          # [null], [int]
    predict_ic_list: null           # [null], [list(int)]

###########################################################################################################################
# 73 channels + Q base
###########################################################################################################################

# 1 GPUS, h=1, no cuda graphs

base_73chq: &BASELINE_73CHQ
    <<: *BASE_CONFIG

base_73chq_finetune: &BASELINE_73CHQ_FINETUNE
    <<: *BASELINE_73CHQ
    finetune: !!bool True
    lr: 1E-6
    scheduler: "CosineAnnealingLR"
    max_epochs: 20
    scheduler_T_max: 20

# current best single GPU model
sfno_linear_73chq_sc3_layers8_edim384_wstgl2:
    <<: *BASELINE_73CHQ
    wandb_group: "sfno_linear_73chq_sc3_layers8_edim384_wstgl2"

    embed_dim: 384
    num_layers: 8
    scale_factor: 3
    hard_thresholding_fraction: 1.0

    loss: "weighted squared temp-std geometric l2"
    channel_weights: "auto"

sfno_linear_73chq_sc3_layers8_edim384_asgl2: &OUT_OF_BOX_CONFIG
    <<: *BASELINE_73CHQ
    wandb_group: "sfno_linear_73chq_sc3_layers8_edim384_asgl2"

    # log_weights_and_grads: 10

    embed_dim: 384
    num_layers: 8
    scale_factor: 3
    hard_thresholding_fraction: 1.0

    optimizer_type: "Adam"

    loss: "absolute squared geometric l2"

small_test1:
    <<: *OUT_OF_BOX_CONFIG
    metadata_json_path: "./datasets/era5/small_test1.json"
    train_data_path: "./datasets/small_test1/train"
    valid_data_path: "./datasets/small_test1/test"
    exp_dir: "./runs/tests"
    n_eval_samples: 500
    max_epochs: 1
    n_train_samples_per_epoch: 100

large_test1_Chou:
    <<: *OUT_OF_BOX_CONFIG
    metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/era5/data_fcn1Vars20.json"
    train_data_path: "/work/choutilin1/out_large_test1/train"
    valid_data_path: "/work/choutilin1/out_large_test1/test"
    inf_data_path: "/work/choutilin1/out_large_test1/inf"
    global_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_means.npy"
    global_stds_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_stds.npy"
    time_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/time_means.npy"
    exp_dir: "/work/choutilin1/out_large_test1"
    n_eval_samples: 200 # for validation
    max_epochs: 7
    n_train_samples_per_epoch: 1460  # for training

large_test2_Chou:
    <<: *OUT_OF_BOX_CONFIG
    num_layers: 6  # hyperparameter trial and error go!!!!!  #choutilin 250623
    metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/era5/data_fcn1Vars20.json"
    train_data_path: "/work/choutilin1/out_large_test1/train"
    valid_data_path: "/work/choutilin1/out_large_test1/test"
    inf_data_path: "/work/choutilin1/out_large_test1/inf"
    global_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_means.npy"
    global_stds_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_stds.npy"
    time_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/time_means.npy"
    exp_dir: "/work/choutilin1/out_large_test1"
    n_eval_samples: 200 # for validation
    max_epochs: 8
    n_train_samples_per_epoch: 1460  # for training

large_test3_Chou:
    <<:
        - *OUT_OF_BOX_CONFIG
        - *PREDICT_BASE
    num_layers: 4  # hyperparameter trial and error go!!!!!  #choutilin 250623
    metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/era5/data_fcn1Vars20.json"
    train_data_path: "/work/choutilin1/out_large_test1/train"
    valid_data_path: "/work/choutilin1/out_large_test1/test"
    inf_data_path: "/work/choutilin1/out_large_test1/inf"
    global_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_means.npy"
    global_stds_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_stds.npy"
    time_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/time_means.npy"
    exp_dir: "/work/choutilin1/out_large_test1"
    n_eval_samples: 200 # for validation
    max_epochs: 9
    n_train_samples_per_epoch: 1460  # for training

    predict_output_overwrite: !!bool True
    predict_output_skipExists: !!bool False
    predict_output_dir: "/work/choutilin1/out_large_test1/"     # [str]
    predict_ic_path: '/work/choutilin1/out_large_test1/inf/2013.h5'
    predict_ic_mode: 'continuous' # ["continuous"]:use start + count, ["incontinuous"]: use list
    predict_ic_start: 0 # [null], [int]
    predict_ic_stop: 1           # [null], [int]
    predict_ic_step: 1             # [null], [int]
    predict_ic_list: null           # [null], [list(int)]

large_test4_Chou:
    <<:
        - *OUT_OF_BOX_CONFIG
#        - *PREDICT_BASE
    num_layers: 4  # hyperparameter trial and error go!!!!!  #choutilin 250623
    metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/era5/data_fcn1Vars20.json"
    train_data_path: "/work/choutilin1/out_large_test4/train"
    valid_data_path: "/work/choutilin1/out_large_test4/test"
    inf_data_path: "/work/choutilin1/out_large_test4/inf"
    global_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_means.npy"
    global_stds_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_stds.npy"
    time_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/time_means.npy"
    exp_dir: "/work/choutilin1/out_large_test4"
    n_eval_samples: 200 # for validation
    max_epochs: 25
    n_train_samples_per_epoch: 1460  # for training

large_test5_Chou:
    <<:
        - *OUT_OF_BOX_CONFIG
#        - *PREDICT_BASE
    num_layers: 8  # hyperparameter trial and error go!!!!!  #choutilin 250623
    metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/era5/data_fcn1Vars20.json"
    train_data_path: "/work/choutilin1/out_large_test4/train"
    valid_data_path: "/work/choutilin1/out_large_test4/test"
    inf_data_path: "/work/choutilin1/out_large_test4/inf"
    global_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_means.npy"
    global_stds_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/global_stds.npy"
    time_means_path: "/work/lkkbox945/models/makani/v250508/datasets/stats/fcn1Vars20/time_means.npy"
    exp_dir: "/work/choutilin1/out_large_test4"
    n_eval_samples: 200 # for validation
    max_epochs: 7
    n_train_samples_per_epoch: 1460  # for training


ehv05a_y1:
    <<: *OUT_OF_BOX_CONFIG
    metadata_json_path: "./datasets/ehv05a_y1/data.json"
    time_means_path:   "./datasets/source/era5_hycom_vars05_a/time_means_2011-2012.npy"
    global_means_path: "./datasets/source/era5_hycom_vars05_a/global_means_2011-2012.npy"
    global_stds_path:  "./datasets/source/era5_hycom_vars05_a/global_stds_2011-2012.npy"
    train_data_path: "./datasets/ehv05a_y1/train"
    valid_data_path: "./datasets/ehv05a_y1/valid"
    exp_dir: "./runs/ehv05a_y1_01"
    max_epochs: 1
    n_train_samples_per_epoch: 100  # for training
    n_eval_samples: 100 # for validation
    inf_data_path: "./datasets/ehv05a_y1/inference"

ehv05a_y3a_Chou:
    <<: 
        - *OUT_OF_BOX_CONFIG
    #    - *PREDICT_BASE
    exp_dir: "/work/choutilin1/out_ehv05a_y3a"

    metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/ehv05a_y3a/data.json"

    time_means_path:   "/work/lkkbox945/models/makani/v250508/datasets/ehv05a_y3a/stat/time_means_2011-2014.npy"
    global_means_path:   "/work/lkkbox945/models/makani/v250508/datasets/ehv05a_y3a/stat/global_means_2011-2014.npy"
    global_stds_path:   "/work/lkkbox945/models/makani/v250508/datasets/ehv05a_y3a/stat/global_stds_2011-2014.npy"

    train_data_path: "/work/choutilin1/out_ehv05a_y3a/train"
    valid_data_path: "/work/choutilin1/out_ehv05a_y3a/valid"

    max_epochs: 1
    n_train_samples_per_epoch: 8 #256  # for training
    n_eval_samples: 4 #64 # for validation
    inf_data_path: "/work/choutilin1/out_ehv05a_y3a/inference"

    predict_output_overwrite: !!bool False
    predict_output_skipExists: !!bool True
    predict_output_dir: "/work/choutilin1/out_ehv05a_y3a/"     # [str]
    predict_ic_path: '/work/lkkbox945/data/era5_hycom_vars05_a/2015.nc'
    predict_ic_mode: 'continuous' # ["continuous"]:use start + count, ["incontinuous"]: use list
    predict_ic_start: 0 # [null], [int]
    predict_ic_stop: 1           # [null], [int]
    predict_ic_step: 1             # [null], [int]
    predict_ic_list: null           # [null], [list(int)]



vars22a_Chou:
    <<: *OUT_OF_BOX_CONFIG
    num_layers: 8  # we are getting CUDA out of memory issues!???  #choutilin 250623
    valid_autoreg_steps: 0 #19  # choutilin 250708: Set this to 1 or 0 during training, otherwise you will OOM
    optimizer_type: "AdamW"
    #metadata_json_path: "/work/lkkbox945/models/makani/v250508/datasets/era5/data_fcn1Vars20.json"
    metadata_json_path: "/work/choutilin1/out_vars22a/fcn1Vars20_vars22a.json"
    train_data_path: "/work/choutilin1/out_vars22a/train"
    valid_data_path: "/work/choutilin1/out_vars22a/test"
    inf_data_path: "/work/choutilin1/out_vars22a/inf"
    global_means_path: "/work/choutilin1/out_vars22a/global_means_2011-2014.npy"
    global_stds_path: "/work/choutilin1/out_vars22a/global_stds_2011-2014.npy"
    time_means_path: "/work/choutilin1/out_vars22a/time_means_2011-2014.npy"
    exp_dir: "/work/choutilin1/out_vars22a"
    n_eval_samples: 500 # for validation
    max_epochs: 10
    n_train_samples_per_epoch: 4380  # for training



trash:
    <<: *OUT_OF_BOX_CONFIG
    num_layers: 8  # we are getting CUDA out of memory issues!???  #choutilin 250623
    valid_autoreg_steps: 19  # choutilin 250708: Set this to 1 or 0 during training, otherwise you will OOM
    log_to_wandb: !!bool True
    optimizer_type: "AdamW"
    metadata_json_path: "/work/choutilin1/out_vars22a/fcn1Vars20_vars22a.json"
    train_data_path: "/work/choutilin1/data_4d/train"
    valid_data_path: "/work/choutilin1/data_4d/test"
    inf_data_path: "/work/choutilin1/data_4d/inf"
    global_means_path: "/work/choutilin1/out_vars22a/global_means_2011-2014.npy"
    global_stds_path: "/work/choutilin1/out_vars22a/global_stds_2011-2014.npy"
    time_means_path: "/work/choutilin1/out_vars22a/time_means_2011-2014.npy"
    exp_dir: "/work/choutilin1/trash"
    n_eval_samples: 500 # for validation
    max_epochs: 3
    n_train_samples_per_epoch: 4380  # for training
